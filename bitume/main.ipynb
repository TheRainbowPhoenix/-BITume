{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# BITume - Experiments"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "    1. import\n",
    "    2. merge\n",
    "    3. prev > dataframe\n",
    "    4. étude de la distribution des données\n",
    "    5. Normalisation\n",
    "    6. Sous-sampling et sur-sempling\n",
    "    7. fonctions d'exploitation des modèles\n",
    "        - param : modèle + normalisation\n",
    "        - indicateurs de classification\n",
    "        - validation croisée\n",
    "    8. lancement des fonctions sur les modèles + exploitation\n",
    "        - une cellule par modèle\n",
    "        - Dummy\n",
    "        - Gaussien\n",
    "            + données pas normalisées\n",
    "                * sous-samplées\n",
    "                * sur-samplées\n",
    "            + données normalisées\n",
    "                * sous-samplées\n",
    "                * sur-samplées\n",
    "        - Decision tree\n",
    "            + données pas normalisées\n",
    "                * sous-samplées\n",
    "                * sur-samplées\n",
    "            + données normalisées\n",
    "                * sous-samplées\n",
    "                * sur-samplées\n",
    "        - Linear Regression\n",
    "            + données pas normalisées\n",
    "                * sous-samplées\n",
    "                * sur-samplées\n",
    "            + données normalisées\n",
    "                * sous-samplées\n",
    "                * sur-samplées\n",
    "        - SVM\n",
    "            + données pas normalisées\n",
    "                * sous-samplées\n",
    "                * sur-samplées\n",
    "            + données normalisées\n",
    "                * sous-samplées\n",
    "                * sur-samplées\n",
    "        - ... ?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# Scoring\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Pre-proc\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Data Balance\n",
    "\"\"\"\n",
    "`pip install imbalanced-learn`\n",
    "Ref: https://towardsdatascience.com/how-to-balance-a-dataset-in-python-36dff9d12704\n",
    "\"\"\"\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.under_sampling import NearMiss\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Utils\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Merging des tables\n",
    "Il s'agit d'une étape optionnelle.\n",
    "Il est important de lancer les cellues dans l'ordre car elles sont dépendantes entre elles"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Lecture des tables et génération de la table de base result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Merge\n",
    "import datetime\n",
    "\n",
    "tbl_df = pd.read_csv('../samples/data/data_mining_DB_clients_tbl.csv')\n",
    "bis_df = pd.read_csv('../samples/data/data_mining_DB_clients_tbl_bis.csv')\n",
    "\n",
    "result = pd.concat([tbl_df, bis_df], ignore_index=True, sort=False)\n",
    "\n",
    "# Consolidation de ANNEE_DEM\n",
    "result['ANNEE_DEM'] = np.where(result['DTDEM'] != '1900-12-31', result['DTDEM'].apply(lambda x: int(x.split('-')[0])), result['ANNEE_DEM'])\n",
    "\n",
    "# Consolidation de AGEAD\n",
    "result['AGEAD'] = np.where(result['AGEAD'].isnull() & result['DTNAIS'].notnull(), (result['DTADH'].apply(lambda x: int(x.split('-')[0]))) - (result['DTNAIS'].apply(lambda x: 0 if type(x) == float else int(x.split('-')[0]))), result['AGEAD'])\n",
    "\n",
    "# Application de la médianne de AGEAD pour valeurs aberrantes\n",
    "result['AGEAD'] = np.where(result['AGEAD'], result['AGEAD'].apply(lambda x: int(result['AGEAD'].median()) if x > 500 else x), result['AGEAD'])\n",
    "\n",
    "# Nettoyage et regénération des ID\n",
    "result = result.drop(['Id'], axis=1)\n",
    "result.insert(0, 'Id', result.index + 1)\n",
    "\n",
    "# Consolidation de adh\n",
    "result['adh'] = np.where(\n",
    "    result['adh'].isnull(),  # & result['DTDEM'] != '1900-12-31'\n",
    "    (result['DTDEM'].apply(lambda x: int(x.split('-')[0]))) - (result['DTADH'].apply(lambda x: 0 if type(x) == float else int(x.split('-')[0]))),\n",
    "    result['adh']\n",
    ")\n",
    "\n",
    "# Factualisation des valeurs, considérant que adh est 2007 pour les non-démissionnaires\n",
    "# 2007 - 1900 = 107\n",
    "result['adh'] = np.where(result['adh'], result['adh'].apply(lambda x: 107 + x if x < 0 else x), result['adh'])\n",
    "\n",
    "# Consolidation de agedem\n",
    "result['agedem'] = np.where(result['agedem'].isnull() & result['DTDEM'].notnull() & result['DTNAIS'].notnull(),\n",
    "                            (result['DTDEM'].apply(lambda x: int(x.split('-')[0]))) - (result['DTNAIS'].apply(lambda x: 0 if type(x) == float else int(x.split('-')[0]))),\n",
    "                            result['agedem'])\n",
    "\n",
    "result['agedem'] = np.where(result['agedem'], result['agedem'].apply(lambda x: 0 if x < 0 else x), result['agedem'])\n",
    "result['agedem'] = np.where(result['agedem'], result['agedem'].apply(lambda x: 0 if x == 1900 else x), result['agedem'])\n",
    "\n",
    "# Nettoyage de la structure\n",
    "result = result.drop(['rangdem'], axis=1)\n",
    "result = result.drop(['rangadh'], axis=1)\n",
    "result = result.drop(['rangagead'], axis=1)\n",
    "result = result.drop(['rangagedem'], axis=1)\n",
    "\n",
    "# Sauvegarde en CSV\n",
    "result.to_csv('result.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Création du fichier de prédiction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Utilisation de la table de résultats nettoyée\n",
    "prev = result\n",
    "\n",
    "# Création de l'indexe de prévision (Y)\n",
    "date_1901 = datetime.datetime(year=1901, month=1, day=1)\n",
    "prev['dem'] = ((~ prev['CDDEM'].isnull()) | (prev['DTDEM'] != '1900-12-31') | (~ prev['ANNEE_DEM'].isnull())).astype(int)\n",
    "\n",
    "# Déplacement de la colonne de prédiction en première position\n",
    "prev.insert(0, 'dem', prev.pop(\"dem\"))\n",
    "\n",
    "# Suppression des tables inutiles, comme expliqué dans le rapport\n",
    "prev = prev.drop(['Id'], axis=1)\n",
    "prev = prev.drop(['DTADH'], axis=1)\n",
    "\n",
    "prev = prev.drop(['CDDEM'], axis=1)\n",
    "prev = prev.drop(['DTDEM'], axis=1)\n",
    "prev = prev.drop(['ANNEE_DEM'], axis=1)\n",
    "prev = prev.drop(['CDMOTDEM'], axis=1)\n",
    "prev = prev.drop(['agedem'], axis=1)\n",
    "prev = prev.drop(['DTNAIS'], axis=1)\n",
    "# Cette col est Incomplete\n",
    "prev = prev.drop(['Bpadh'], axis=1)\n",
    "\n",
    "# Cette col n'est pas utile en l'état\n",
    "prev = prev.drop(['adh'], axis=1)\n",
    "\n",
    "# Correction des types\n",
    "prev['AGEAD'] = prev['AGEAD'].astype(int)\n",
    "\n",
    "prev.to_csv('previsions_nd.csv', index=False)\n",
    "\n",
    "# disjonction\n",
    "dummies = ['CDTMT', 'CDSITFAM', 'CDTMT', 'CDCATCL']\n",
    "prev = pd.get_dummies(prev, prefix=dummies, columns=dummies)\n",
    "\n",
    "# Sauvegarde en CSV pour exploitation\n",
    "prev.to_csv('previsions.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exportation en base de données\n",
    "L'exportation en base SQL permet de réaliser des traitement génériques SQL sur les données  "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine('sqlite:///clients_tbl.db', echo=False)\n",
    "\n",
    "tbl_df.to_sql('clients', con=engine, if_exists='replace')\n",
    "bis_df.to_sql('bis', con=engine, if_exists='replace')\n",
    "result.to_sql('result', con=engine, if_exists='replace')\n",
    "prev.to_sql('prev', con=engine, if_exists='replace')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Importation des données\n",
    "Cette étape a pour but de restorer les données en l'état depuis le fichier CSV généré"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Importer dans une dataframe pandas le CSV\n",
    "prev_df = pd.read_csv('previsions.csv', delimiter=',')\n",
    "# Afficher son contenu pour s'assurer qu'il est correctement lu\n",
    "prev_df.head()\n",
    "\n",
    "# Conversion de la dataframe en numpy pour calculuer la moyenne et les X / Y\n",
    "users_array = prev_df.to_numpy()\n",
    "# Calcul de la \"drop rate\" moyenne, le taut de démission\n",
    "average_drop_rate = (np.mean([i[0] for i in users_array]) * 100)\n",
    "\n",
    "# Séparation des variables (X) de la cible de prévision (Y)\n",
    "X = users_array[:, 1:]\n",
    "Y = users_array[:, 0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Étude de la distribution des données\n",
    "Cette étape permet de constater s'il existe des disparités dans la distribution des résultats.\n",
    "\n",
    "Une source d'inspiration a été [l'article suivant](https://machinelearningmastery.com/undersampling-algorithms-for-imbalanced-classification/)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution normale : Counter({1: 30880, 0: 14474})\n",
      "[1.56674036 0.73435881]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hp\\documents\\github\\bitume\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass classes=[0 1], y=[1 1 1 ... 0 0 0] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    }
   ],
   "source": [
    "# Mise en place d'un compteur de distribution\n",
    "counter = Counter(Y)\n",
    "\n",
    "# Décompte des valeurs de la cible\n",
    "print(f\"Distribution normale : {counter}\")\n",
    "\n",
    "from sklearn.utils import compute_class_weight\n",
    "# Calcul du poids des classes\n",
    "class_weight = compute_class_weight('balanced', np.unique(Y), Y)\n",
    "\n",
    "print(class_weight)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Normalisation\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "min_max = preprocessing.MinMaxScaler()\n",
    "X_minmax = min_max.fit_transform(X)\n",
    "\n",
    "std_scale = preprocessing.StandardScaler()\n",
    "X_scale = std_scale.fit_transform(X)\n",
    "\n",
    "X_l1 = preprocessing.normalize(X, norm=\"l1\")\n",
    "X_l2 = preprocessing.normalize(X, norm=\"l2\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sous-sampling et sur-sempling\n",
    "\n",
    "## NearMiss\n",
    "Le principe du NearMiss est de réaliser un sous-échantillonage. La référence se trouve à [l'adresse suivante](https://imbalanced-learn.org/stable/under_sampling.html)\n",
    "\n",
    "Note: l'execution prends trop de temps pour mon PC"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-8-6356f7e85f63>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[0mX_nearmiss_scale\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mY_nearmiss_scale\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnm1\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit_resample\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX_scale\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mY\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 9\u001B[1;33m \u001B[0mX_nearmiss_l1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mY_nearmiss_l1\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnm1\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit_resample\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX_l1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mY\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     10\u001B[0m \u001B[0mX_nearmiss_l2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mY_nearmiss_l2\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnm1\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit_resample\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX_l2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mY\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\hp\\documents\\github\\bitume\\venv\\lib\\site-packages\\imblearn\\base.py\u001B[0m in \u001B[0;36mfit_resample\u001B[1;34m(self, X, y)\u001B[0m\n\u001B[0;32m     81\u001B[0m         )\n\u001B[0;32m     82\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 83\u001B[1;33m         \u001B[0moutput\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_fit_resample\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     84\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     85\u001B[0m         \u001B[0my_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlabel_binarize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutput\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0munique\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mbinarize_y\u001B[0m \u001B[1;32melse\u001B[0m \u001B[0moutput\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\hp\\documents\\github\\bitume\\venv\\lib\\site-packages\\imblearn\\under_sampling\\_prototype_selection\\_nearmiss.py\u001B[0m in \u001B[0;36m_fit_resample\u001B[1;34m(self, X, y)\u001B[0m\n\u001B[0;32m    219\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    220\u001B[0m                 \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mversion\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 221\u001B[1;33m                     dist_vec, idx_vec = self.nn_.kneighbors(\n\u001B[0m\u001B[0;32m    222\u001B[0m                         \u001B[0mX_class\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mn_neighbors\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnn_\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mn_neighbors\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    223\u001B[0m                     )\n",
      "\u001B[1;32mc:\\users\\hp\\documents\\github\\bitume\\venv\\lib\\site-packages\\sklearn\\neighbors\\_base.py\u001B[0m in \u001B[0;36mkneighbors\u001B[1;34m(self, X, n_neighbors, return_distance)\u001B[0m\n\u001B[0;32m    703\u001B[0m                 \u001B[0mkwds\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0meffective_metric_params_\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    704\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 705\u001B[1;33m             chunked_results = list(pairwise_distances_chunked(\n\u001B[0m\u001B[0;32m    706\u001B[0m                 \u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_fit_X\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreduce_func\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mreduce_func\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    707\u001B[0m                 \u001B[0mmetric\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0meffective_metric_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mn_jobs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mn_jobs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\hp\\documents\\github\\bitume\\venv\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001B[0m in \u001B[0;36mpairwise_distances_chunked\u001B[1;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001B[0m\n\u001B[0;32m   1621\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1622\u001B[0m             \u001B[0mX_chunk\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mX\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0msl\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1623\u001B[1;33m         D_chunk = pairwise_distances(X_chunk, Y, metric=metric,\n\u001B[0m\u001B[0;32m   1624\u001B[0m                                      n_jobs=n_jobs, **kwds)\n\u001B[0;32m   1625\u001B[0m         if ((X is Y or Y is None)\n",
      "\u001B[1;32mc:\\users\\hp\\documents\\github\\bitume\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py\u001B[0m in \u001B[0;36minner_f\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     61\u001B[0m             \u001B[0mextra_args\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mall_args\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     62\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mextra_args\u001B[0m \u001B[1;33m<=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 63\u001B[1;33m                 \u001B[1;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     64\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     65\u001B[0m             \u001B[1;31m# extra_args > 0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\hp\\documents\\github\\bitume\\venv\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001B[0m in \u001B[0;36mpairwise_distances\u001B[1;34m(X, Y, metric, n_jobs, force_all_finite, **kwds)\u001B[0m\n\u001B[0;32m   1788\u001B[0m         \u001B[0mfunc\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpartial\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdistance\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcdist\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmetric\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmetric\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1789\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1790\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0m_parallel_pairwise\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mY\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mn_jobs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1791\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1792\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\hp\\documents\\github\\bitume\\venv\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001B[0m in \u001B[0;36m_parallel_pairwise\u001B[1;34m(X, Y, func, n_jobs, **kwds)\u001B[0m\n\u001B[0;32m   1357\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1358\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0meffective_n_jobs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mn_jobs\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1359\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mY\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1360\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1361\u001B[0m     \u001B[1;31m# enforce a threading backend to prevent data communication overhead\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\hp\\documents\\github\\bitume\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py\u001B[0m in \u001B[0;36minner_f\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     61\u001B[0m             \u001B[0mextra_args\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mall_args\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     62\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mextra_args\u001B[0m \u001B[1;33m<=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 63\u001B[1;33m                 \u001B[1;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     64\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     65\u001B[0m             \u001B[1;31m# extra_args > 0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\hp\\documents\\github\\bitume\\venv\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001B[0m in \u001B[0;36meuclidean_distances\u001B[1;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001B[0m\n\u001B[0;32m    314\u001B[0m         \u001B[0mdistances\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0mXX\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    315\u001B[0m         \u001B[0mdistances\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0mYY\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 316\u001B[1;33m     \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmaximum\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdistances\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mout\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdistances\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    317\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    318\u001B[0m     \u001B[1;31m# Ensure that distances between vectors and themselves are set to 0.0.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import NearMiss\n",
    "\n",
    "# Création d'un resampler NearMiss 1\n",
    "nm1 = NearMiss(version=1)\n",
    "# Équilibrage des échantillions\n",
    "X_nearmiss, Y_nearmiss = nm1.fit_resample(X, Y)\n",
    "\n",
    "X_nearmiss_scale, Y_nearmiss_scale = nm1.fit_resample(X_scale, Y)\n",
    "X_nearmiss_l1, Y_nearmiss_l1 = nm1.fit_resample(X_l1, Y)\n",
    "X_nearmiss_l2, Y_nearmiss_l2 = nm1.fit_resample(X_l2, Y)\n",
    "\n",
    "# Mise en place d'un compteur de distribution\n",
    "counter = Counter(Y_nearmiss)\n",
    "\n",
    "# Décompte des valeurs de la cible\n",
    "print(f\"Sous-échantillonnage / distribution NearMiss : {counter}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SMOTE\n",
    "Le principe de la Technique de sur-échantillonnage des Minorités Synthétiques (SMOTE) est de réaliser un sur-échantillonage.\n",
    "\n",
    "La référence se trouve à [l'adresse suivante](https://imbalanced-learn.org/stable/over_sampling.html)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sm = SMOTE()\n",
    "# Création d'un resampler et équilibrage des échantillions\n",
    "X_smote, Y_smote = sm.fit_resample(X, Y)\n",
    "\n",
    "X_smote_scale, Y_smote_scale = sm.fit_resample(X_scale, Y)\n",
    "X_smote_l1, Y_smote_l1 = sm.fit_resample(X_l1, Y)\n",
    "X_smote_l2, Y_smote_l2 = sm.fit_resample(X_l2, Y)\n",
    "\n",
    "# Mise en place d'un compteur de distribution\n",
    "counter = Counter(Y_smote)\n",
    "\n",
    "# Décompte des valeurs de la cible\n",
    "print(f\"Sur-échantillonnage / distribution SMOTE : {counter}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ADASYN\n",
    "Le principe de l'Adaptation Synthétique (ADASYN) est de réaliser un sur-échantillonage.\n",
    "\n",
    "La référence se trouve à [l'adresse suivante](https://imbalanced-learn.org/stable/over_sampling.html)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "asn = ADASYN()\n",
    "\n",
    "X_as, Y_as = asn.fit_resample(X, Y)\n",
    "\n",
    "X_as_scale, Y_as_scale = asn.fit_resample(X_scale, Y)\n",
    "X_as_l1, Y_as_l1 = asn.fit_resample(X_l1, Y)\n",
    "X_as_l2, Y_as_l2 = asn.fit_resample(X_l2, Y)\n",
    "\n",
    "# Mise en place d'un compteur de distribution\n",
    "counter = Counter(Y_as)\n",
    "\n",
    "# Décompte des valeurs de la cible\n",
    "print(f\"Sur-échantillonnage / distribution ADASYN : {counter}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fonctions d'exploitation des modèles"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def calc_accuracy_score(model, name: str, p_X=X, p_Y=Y):\n",
    "    scores = cross_val_score(model, p_X, p_Y, cv=10)\n",
    "    scores_precision = cross_val_score(model, p_X, p_Y, cv=10, scoring='precision')\n",
    "    scores_recall = cross_val_score(model, p_X, p_Y, cv=10, scoring='recall')\n",
    "    scores_f1 = cross_val_score(model, p_X, p_Y, cv=10, scoring='f1')\n",
    "    print(f\"\"\"\n",
    "Évaluation de {name} sous Validation Croisée 10-étapes\n",
    "\n",
    "Exactitude   : {scores.mean():.3f} (+/- {scores.std() * 2:.3f})\n",
    "Précision    : {scores_precision.mean():.3f} (+/- {scores_precision.std() * 2:.3f})\n",
    "Ratio Rappel : {scores_recall.mean():.3f} (+/- {scores_recall.std() * 2:.3f})\n",
    "F-score      : {scores_f1.mean():.3f} (+/- {scores_f1.std() * 2:.3f})\n",
    "    \"\"\")\n",
    "\n",
    "def calc_model_score(pipeline, p_x_test, p_y_test, name: str):\n",
    "    print(f'Précision de {name} {pipeline.score(p_x_test, p_y_test)}')\n",
    "\n",
    "def calc_MAE_MSE_EMSE_from(prediction, p_y_test, name):\n",
    "    print(f\"MAE sur {name} {mean_absolute_error(p_y_test, prediction)}\")\n",
    "    print(f\"MSE sur {name} {mean_squared_error(p_y_test, prediction)}\")\n",
    "    print(f\"RMSE sur {name} {np.sqrt(mean_squared_error(p_y_test, prediction))}\")\n",
    "    print(\"\")\n",
    "\n",
    "def get_model_data(model_instance, X_dat=X, Y_dat=Y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_dat, Y_dat, test_size=0.2, random_state=42)\n",
    "    pipeline = make_pipeline(model_instance)\n",
    "    model = pipeline.fit(X_train, y_train)\n",
    "    prediction = model.predict(X_test)\n",
    "    return X_test, y_test, model, prediction, pipeline\n",
    "\n",
    "def report_model(p_model, name:str, X_dat=X, Y_dat=Y):\n",
    "    X_test, y_test, model, prediction, pipeline = get_model_data(p_model, X_dat, Y_dat)\n",
    "    calc_model_score(pipeline, X_test, y_test, name)\n",
    "    calc_accuracy_score(model, name)\n",
    "    calc_MAE_MSE_EMSE_from(prediction, y_test, name)\n",
    "\n",
    "def report_all_model(p_model, name:str):\n",
    "    report_model(p_model, name)\n",
    "    print(\"Et diminuées\\n=============\\n\")\n",
    "    report_model(p_model, name+\" NearMiss\", X_dat=X_nearmiss, Y_dat=Y_nearmiss)\n",
    "    print(\"\\n\\nEt augmentées\\n=============\\n\")\n",
    "    report_model(p_model, name+\" SMOTE\", X_dat=X_smote, Y_dat=Y_smote)\n",
    "    report_model(p_model, name+\" ADASYN\", X_dat=X_as, Y_dat=Y_as)\n",
    "\n",
    "    print(\"\\n\\nAvec des données normalisées\")\n",
    "    print(\"Et non-resamplées\\n=============\\n\")\n",
    "\n",
    "    report_model(p_model, name+\" avec Répartitions Normales\", X_dat=X_scale, Y_dat=Y)\n",
    "    report_model(p_model, name+\" avec Normalisation L1\", X_dat=X_l1, Y_dat=Y)\n",
    "    report_model(p_model, name+\" avec Normalisation L2\", X_dat=X_l2, Y_dat=Y)\n",
    "    print(\"\\n\\nEt diminuées\\n=============\\n\")\n",
    "    report_model(p_model, name+\" NearMiss avec Répartitions Normales\", X_dat=X_nearmiss_scale, Y_dat=Y_nearmiss_scale)\n",
    "    report_model(p_model, name+\" NearMiss avec Normalisation L1\", X_dat=X_nearmiss_l1, Y_dat=Y_nearmiss_l1)\n",
    "    report_model(p_model, name+\" NearMiss avec Normalisation L2\", X_dat=X_nearmiss_l2, Y_dat=Y_nearmiss_l2)\n",
    "    print(\"\\n\\nEt augmentées\\n=============\\n\")\n",
    "    report_model(p_model, name+\" SMOTE avec Répartitions Normales\", X_dat=X_smote_scale, Y_dat=Y_smote_scale)\n",
    "    report_model(p_model, name+\" SMOTE avec Normalisation L1\", X_dat=X_smote_l1, Y_dat=Y_smote_l1)\n",
    "    report_model(p_model, name+\" SMOTE avec Normalisation L2\", X_dat=X_smote_l2, Y_dat=Y_smote_l2)\n",
    "\n",
    "    report_model(p_model, name+\" ADASYN avec Répartitions Normales\", X_dat=X_as_scale, Y_dat=Y_as_scale)\n",
    "    report_model(p_model, name+\" ADASYN avec Normalisation L1\", X_dat=X_as_l1, Y_dat=Y_as_l1)\n",
    "    report_model(p_model, name+\" ADASYN avec Normalisation L2\", X_dat=X_as_l2, Y_dat=Y_as_l2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Étude des modèles\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## le Dummy Classifier\n",
    "Le dummy classfier est un classifier naif qui estime à coup de fréquence la valeur"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Mise en place d'un DummyClassifier permettant d'évaluer les performances\n",
    "# des autres algoritmes\n",
    "dummycl_model = DummyClassifier(strategy=\"most_frequent\")\n",
    "\n",
    "name = \"le Dummy Classifier\"\n",
    "report_model(dummycl_model, name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## le Gaussien\n",
    "Le Gaussien est un modèle probabiliste sur une distribution normale, qui suit la formule\n",
    "`P(class|data) = ( P(data|class) x P(class) ) / P(data)`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Mise en place d'une classification naïve bayésienne\n",
    "gmb_model = GaussianNB()\n",
    "\n",
    "name = \"le Biais Natif Gaussien\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Avec des données non normalisées\n",
    "#### Et non-resamplées"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "report_model(gmb_model, name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Et diminuées"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "report_model(gmb_model, name+\" NearMiss\", X_dat=X_nearmiss, Y_dat=Y_nearmiss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Et augmentées"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "report_model(gmb_model, name+\" SMOTE\", X_dat=X_smote, Y_dat=Y_smote)\n",
    "report_model(gmb_model, name+\" ADASYN\", X_dat=X_as, Y_dat=Y_as)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Avec des données normalisées\n",
    "#### Et non-resamplées"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "report_model(gmb_model, name+\" avec Répartitions Normales\", X_dat=X_scale, Y_dat=Y)\n",
    "report_model(gmb_model, name+\" avec Normalisation L1\", X_dat=X_l1, Y_dat=Y)\n",
    "report_model(gmb_model, name+\" avec Normalisation L2\", X_dat=X_l2, Y_dat=Y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Et diminuées"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "report_model(gmb_model, name+\" NearMiss avec Répartitions Normales\", X_dat=X_nearmiss_scale, Y_dat=Y_nearmiss_scale)\n",
    "report_model(gmb_model, name+\" NearMiss avec Normalisation L1\", X_dat=X_nearmiss_l1, Y_dat=Y_nearmiss_l1)\n",
    "report_model(gmb_model, name+\" NearMiss avec Normalisation L2\", X_dat=X_nearmiss_l2, Y_dat=Y_nearmiss_l2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Et augmentées"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "report_model(gmb_model, name+\" SMOTE avec Répartitions Normales\", X_dat=X_smote_scale, Y_dat=Y_smote_scale)\n",
    "report_model(gmb_model, name+\" SMOTE avec Normalisation L1\", X_dat=X_smote_l1, Y_dat=Y_smote_l1)\n",
    "report_model(gmb_model, name+\" SMOTE avec Normalisation L2\", X_dat=X_smote_l2, Y_dat=Y_smote_l2)\n",
    "\n",
    "report_model(gmb_model, name+\" ADASYN avec Répartitions Normales\", X_dat=X_as_scale, Y_dat=Y_as_scale)\n",
    "report_model(gmb_model, name+\" ADASYN avec Normalisation L1\", X_dat=X_as_l1, Y_dat=Y_as_l1)\n",
    "report_model(gmb_model, name+\" ADASYN avec Normalisation L2\", X_dat=X_as_l2, Y_dat=Y_as_l2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ====================================================="
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## l'Arbre de décision\n",
    "L'Arbre de décision est un modèle de décision probabiliste basé sur un apprentissage et\n",
    "considérant une faible variation entre les données d'apprentissage et de test.\n",
    "Il se révèle particulièrement performant face à des cas déjà connus."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "# Mise en place d'un arbre de décision\n",
    "dectree_model = tree.DecisionTreeClassifier()\n",
    "\n",
    "name = \"l'Arbre de décision\"\n",
    "\n",
    "report_all_model(dectree_model, name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## la Régression Linéaire\n",
    "La Régression Linéaire est une manière de prédire les données en les séparant en plusieurs classes\n",
    "Elle est connue pour rapidement donner des résultats cohérents"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Mise en place d'un modèle de régression linéaire, connu pour\n",
    "# avoir un important rapport performance / temps d'execution\n",
    "lr_model = LogisticRegression()\n",
    "\n",
    "name = \"la Régression Linéaire\"\n",
    "report_all_model(lr_model, name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## la Classification Supporté par les Vecteurs\n",
    "La Classification Supporté par les Vecteurs est très intense pour mon toaster\n",
    "### Avec LinearSVC"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Le SVC avec un meilleur suppot sur les rgands dataset\n",
    "lsvc_model = LinearSVC()\n",
    "\n",
    "name = \"la Classification Linéaire Supporté par les Vecteurs\"\n",
    "\n",
    "report_all_model(lsvc_model, name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Avec SVC (libsvm)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Mise en place d'un Support Vecteur Machine, présentant\n",
    "# en général de meilleurs performances\n",
    "svm_model = SVC()\n",
    "\n",
    "name = \"la Classification Supporté par les Vecteurs\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}