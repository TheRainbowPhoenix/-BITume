{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Id  CDSEXE  MTREV  NBENF CDSITFAM       DTADH  CDTMT  CDDEM  \\\n",
      "0          1       2      0      0        A  1972-04-04      0      2   \n",
      "1          2       2      0      0        A  1972-04-12      0      2   \n",
      "2          3       2      0      0        B  1972-07-19      0      2   \n",
      "3          4       3      0      0        M  1972-08-28      0      2   \n",
      "4          5       2      0      0        A  1972-11-06      0      2   \n",
      "...      ...     ...    ...    ...      ...         ...    ...    ...   \n",
      "30327  30328       2      0      0        M  1982-06-16      0      2   \n",
      "30328  30329       3      0      0        A  1980-02-29      0      2   \n",
      "30329  30330       3      0      0        A  1980-09-17      0      2   \n",
      "30330  30331       3      0      0        A  1980-07-03      0      2   \n",
      "30331  30332       2      0      0        A  1983-07-29      0      2   \n",
      "\n",
      "            DTDEM  ANNEE_DEM CDMOTDEM  CDCATCL  AGEAD rangagead  agedem  \\\n",
      "0      2003-06-25       2003       DV       21     33  3  31-35      64   \n",
      "1      2005-07-06       2005       DV       21     55  7  51-55      88   \n",
      "2      2001-02-14       2001       DV       21     39  4  36-40      68   \n",
      "3      2006-03-09       2006       DV       21     29  2  26-30      63   \n",
      "4      1999-03-22       1999       DV       21     45  5  41-45      72   \n",
      "...           ...        ...      ...      ...    ...       ...     ...   \n",
      "30327  2005-06-15       2005       DV       21     32  3  31-35      55   \n",
      "30328  2005-06-27       2005       DV       21     36  4  36-40      61   \n",
      "30329  1999-01-12       1999       DA       21     32  3  31-35      51   \n",
      "30330  1999-06-03       1999       DV       21     31  3  31-35      50   \n",
      "30331  2005-01-28       2005       RA       21     40  4  36-40      62   \n",
      "\n",
      "      rangagedem  rangdem  adh   rangadh  \n",
      "0       9  61-65  5  2003   31  7  30-34  \n",
      "1        b  71-+  7  2005   33  7  30-34  \n",
      "2       a  66-70  3  2001   29  6  25-29  \n",
      "3       9  61-65  8  2006   34  7  30-34  \n",
      "4        b  71-+  1  1999   27  6  25-29  \n",
      "...          ...      ...  ...       ...  \n",
      "30327   7  51-55  7  2005   23  5  20-24  \n",
      "30328   9  61-65  7  2005   25  6  25-29  \n",
      "30329   7  51-55  1  1999   19  4  15-19  \n",
      "30330   6  46-50  1  1999   19  4  15-19  \n",
      "30331   9  61-65  7  2005   22  5  20-24  \n",
      "\n",
      "[30332 rows x 19 columns]\n"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "%run testit.py"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Pre-proc\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Data Balance\n",
    "\"\"\"\n",
    "`pip install imbalanced-learn`\n",
    "Ref: https://towardsdatascience.com/how-to-balance-a-dataset-in-python-36dff9d12704\n",
    "\"\"\"\n",
    "from imblearn.pipeline import make_pipeline as make_pipeline_imb\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# Utils\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   dem  CDSEXE  MTREV  NBENF  AGEAD  CDTMT_0  CDTMT_2  CDTMT_4  CDTMT_6  \\\n0    1       2      0      0     33        1        0        0        0   \n1    1       2      0      0     55        1        0        0        0   \n2    1       2      0      0     39        1        0        0        0   \n3    1       3      0      0     29        1        0        0        0   \n4    1       2      0      0     45        1        0        0        0   \n\n   CDSITFAM_A  ...  CDCATCL_22  CDCATCL_23  CDCATCL_24  CDCATCL_25  \\\n0           1  ...           0           0           0           0   \n1           1  ...           0           0           0           0   \n2           0  ...           0           0           0           0   \n3           0  ...           0           0           0           0   \n4           1  ...           0           0           0           0   \n\n   CDCATCL_32  CDCATCL_40  CDCATCL_50  CDCATCL_61  CDCATCL_82  CDCATCL_98  \n0           0           0           0           0           0           0  \n1           0           0           0           0           0           0  \n2           0           0           0           0           0           0  \n3           0           0           0           0           0           0  \n4           0           0           0           0           0           0  \n\n[5 rows x 37 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dem</th>\n      <th>CDSEXE</th>\n      <th>MTREV</th>\n      <th>NBENF</th>\n      <th>AGEAD</th>\n      <th>CDTMT_0</th>\n      <th>CDTMT_2</th>\n      <th>CDTMT_4</th>\n      <th>CDTMT_6</th>\n      <th>CDSITFAM_A</th>\n      <th>...</th>\n      <th>CDCATCL_22</th>\n      <th>CDCATCL_23</th>\n      <th>CDCATCL_24</th>\n      <th>CDCATCL_25</th>\n      <th>CDCATCL_32</th>\n      <th>CDCATCL_40</th>\n      <th>CDCATCL_50</th>\n      <th>CDCATCL_61</th>\n      <th>CDCATCL_82</th>\n      <th>CDCATCL_98</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>33</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>55</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>39</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>29</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>45</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 37 columns</p>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importer dans une dataframe pandas le CSV\n",
    "prev_df = pd.read_csv('previsions.csv', delimiter=',')\n",
    "# Afficher son contenu pour s'assurer qu'il est correctement lu\n",
    "prev_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Conversion de la dataframe en numpy pour calculuer la moyenne et les X / Y\n",
    "users_array = prev_df.to_numpy()\n",
    "# Calcul de la \"drop rate\" moyenne, le taut de démission\n",
    "average_drop_rate = (np.mean([i[0] for i in users_array]) * 100)\n",
    "\n",
    "# Séparation des variables (X) de la cible de prévision (Y)\n",
    "X = users_array[:, 1:]\n",
    "Y = users_array[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Statistiques sur la transormation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-a13bfd130856>:13: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "<ipython-input-13-a13bfd130856>:20: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution normale : Counter({1: 30880, 0: 14474})\n"
     ]
    }
   ],
   "source": [
    "# https://machinelearningmastery.com/undersampling-algorithms-for-imbalanced-classification/\n",
    "\n",
    "# Mise en place d'un compteur de distribution\n",
    "counter = Counter(Y)\n",
    "\n",
    "# Distribution graphique des résultats par deux facteurs discriminés par les valeurs de la cible\n",
    "\n",
    "# Agead / MtRev\n",
    "for label, _ in counter.items():\n",
    "    row_ix = np.where(Y == label)[0]\n",
    "    plt.scatter(X[row_ix, 3], X[row_ix, 1], label=str(label))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Agead / NbEnf\n",
    "for label, _ in counter.items():\n",
    "    row_ix = np.where(Y == label)[0]\n",
    "    plt.scatter(X[row_ix, 3], X[row_ix, 2], label=str(label))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Décompte des valeurs de la cible\n",
    "print(f\"Distribution normale : {counter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.56674036 0.73435881]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hp\\documents\\github\\bitume\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass classes=[0 1], y=[1 1 1 ... 0 0 0] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import compute_class_weight\n",
    "# Calcul du poids des classes\n",
    "class_weight = compute_class_weight('balanced', np.unique(Y), Y)\n",
    "\n",
    "print(class_weight)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Near Miss\n",
    "\n",
    "Le principe du NearMiss est de réaliser un sous-échantillonage.\n",
    "La référence se trouve à [l'adresse suivante](https://imbalanced-learn.org/stable/under_sampling.html)\n",
    "\n",
    "Note: l'execution prends trop de temps pour mon PC"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sous-échantillonnage / distribution NearMiss : Counter({0: 14474, 1: 14474})\n"
     ]
    }
   ],
   "source": [
    "# Création d'un resampler NearMiss 1\n",
    "nm1 = NearMiss(version=1)\n",
    "# Équilibrage des échantillions\n",
    "X_nearmiss, Y_nearmiss = nm1.fit_resample(X, Y)\n",
    "\n",
    "# Mise en place d'un compteur de distribution\n",
    "counter = Counter(Y_nearmiss)\n",
    "\n",
    "# Décompte des valeurs de la cible\n",
    "print(f\"Sous-échantillonnage / distribution NearMiss : {counter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n# Création d\\'un resampler NearMiss 2\\nnm1 = NearMiss(version=2)\\n# Équilibrage des échantillions\\nX_nearmiss2, Y_nearmiss2 = nm1.fit_resample(X, Y)\\n\\n# Mise en place d\\'un compteur de distribution\\ncounter = Counter(Y_nearmiss2)\\n\\n# Décompte des valeurs de la cible\\nprint(f\"Sous-échantillonnage / distribution NearMiss 2 : {counter}\")\\n'"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Création d'un resampler NearMiss 2\n",
    "nm1 = NearMiss(version=2)\n",
    "# Équilibrage des échantillions\n",
    "X_nearmiss2, Y_nearmiss2 = nm1.fit_resample(X, Y)\n",
    "\n",
    "# Mise en place d'un compteur de distribution\n",
    "counter = Counter(Y_nearmiss2)\n",
    "\n",
    "# Décompte des valeurs de la cible\n",
    "print(f\"Sous-échantillonnage / distribution NearMiss 2 : {counter}\")\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SMOTE\n",
    "Le principe de la Technique de sur-échantillonnage des Minorités Synthétiques (SMOTE) est de réaliser un sur-échantillonage.\n",
    "\n",
    "La référence se trouve à [l'adresse suivante](https://imbalanced-learn.org/stable/over_sampling.html)\n",
    "\n",
    "\n",
    "Note: l'execution prends trop de temps pour mon PC"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sur-échantillonnage / distribution SMOTE : Counter({1: 30880, 0: 30880})\n"
     ]
    }
   ],
   "source": [
    "# Création d'un resampler et équilibrage des échantillions\n",
    "X_smote, Y_smote = SMOTE().fit_resample(X, Y)\n",
    "\n",
    "# Mise en place d'un compteur de distribution\n",
    "counter = Counter(Y_smote)\n",
    "\n",
    "# Décompte des valeurs de la cible\n",
    "print(f\"Sur-échantillonnage / distribution SMOTE : {counter}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ADASYN\n",
    "Le principe de l'Adaptation Synthétique (ADASYN) est de réaliser un sur-échantillonage.\n",
    "\n",
    "La référence se trouve à [l'adresse suivante](https://imbalanced-learn.org/stable/over_sampling.html)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sur-échantillonnage / distribution ADASYN : Counter({0: 31270, 1: 30880})\n"
     ]
    }
   ],
   "source": [
    "# Création d'un resampler et équilibrage des échantillions\n",
    "X_as, Y_as = ADASYN().fit_resample(X, Y)\n",
    "\n",
    "# Mise en place d'un compteur de distribution\n",
    "counter = Counter(Y_as)\n",
    "\n",
    "# Décompte des valeurs de la cible\n",
    "print(f\"Sur-échantillonnage / distribution ADASYN : {counter}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import tree\n",
    "\n",
    "# Mise en place d'un modèle de régression linéaire, connu pour\n",
    "# avoir un important rapport performance / temps d'execution\n",
    "lr_model = LogisticRegression\n",
    "\n",
    "# Mise en place d'un Support Vecteur Machine, présentant\n",
    "# en général de meilleurs performances\n",
    "svm_model = SVC()\n",
    "\n",
    "# Mise en place d'un DummyClassifier permettant d'évaluer les performances\n",
    "# des autres algoritmes\n",
    "dummycl = DummyClassifier(strategy=\"most_frequent\")\n",
    "\n",
    "# Mise en place d'une classification naïve bayésienne\n",
    "gmb = GaussianNB()\n",
    "\n",
    "# Mise en place d'un arbre de décision\n",
    "dectree = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modèle 80/20 Régression Linéaire"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score de la Régression Linéaire sans distribution 0.9212876198875537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hp\\documents\\github\\bitume\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Découpage du corpus en apprentissage et test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Création d'une pipeline avec un modèle linéaire\n",
    "pipeline = make_pipeline(lr_model(random_state=42))\n",
    "# Entrainement avec les valeurs d'apprentissage\n",
    "model = pipeline.fit(X_train, y_train)\n",
    "# Calcul de la précision du modèle avec le corpus de test\n",
    "prediction = model.predict(X_test)\n",
    "\n",
    "# Affichage du score\n",
    "print(f'Score de la Régression Linéaire sans distribution {pipeline.score(X_test, y_test)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Les pipelines sont documentées à [l'adresse suivante](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n",
    "\n",
    "<br/>\n",
    "\n",
    "### NearMiss sur modèle 80/20 Régression Linéaire\n",
    "\n",
    "Le modèle actuel présente un ensemble déséquilibré (on possède plus de personnes ayant quitté qu'étant restées).\n",
    "On utilise alors un ensemble de techniques, comme expliqué dans [cet article](https://heartbeat.fritz.ai/resampling-to-properly-handle-imbalanced-datasets-in-machine-learning-64d82c16ceaa)\n",
    "\n",
    "Référence sur les pipelines déséquilibrées à [l'addresse suivante](https://imbalanced-learn.org/stable/under_sampling.html#controlled-under-sampling-techniques)\n",
    "\n",
    "\n",
    "Note: Je n'arrive pas à faire tourner ces algo sur mon pc, ils demandent une trop grande puissance de calcul"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision de la Régression Linéaire avec Sous-échantillonnage NearMiss 0.9381692573402418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hp\\documents\\github\\bitume\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Création d'un resampler NearMiss\n",
    "nm1 = NearMiss(version=1)\n",
    "\n",
    "# Équilibrage des échantillions\n",
    "X_resampled_nm1, Y_resampled = nm1.fit_resample(X, Y)\n",
    "\n",
    "# Découpage du corpus en apprentissage et test\n",
    "NM_X_train, NM_X_test, NM_y_train, NM_y_test = train_test_split(X_resampled_nm1, Y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Création d'une pipeline avec un modèle linéaire\n",
    "pipeline = make_pipeline(lr_model(random_state=42))\n",
    "# Entrainement avec les valeurs d'apprentissage\n",
    "model = pipeline.fit(NM_X_train, NM_y_train)\n",
    "# Calcul de la précision du modèle avec le corpus de test\n",
    "prediction = model.predict(NM_X_test)\n",
    "\n",
    "# Affichage du score\n",
    "print(f'Précision de la Régression Linéaire avec Sous-échantillonnage NearMiss {pipeline.score(NM_X_test, NM_y_test)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SMOTE sur modèle 80/20 Régression Linéaire\n",
    "\n",
    "Référence sur l'utilisation du Sur-échantillonage à [l'addresse suivante](https://imbalanced-learn.org/stable/over_sampling.html#from-random-over-sampling-to-smote-and-adasyn)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision de la Régression Linéaire avec Sur-échantillonnage SMOTE 0.9008257772020726\n"
     ]
    }
   ],
   "source": [
    "# Création et équilibrage des échantillions\n",
    "X_smote, Y_smote = SMOTE().fit_resample(X, Y)\n",
    "\n",
    "# Découpage du corpus en apprentissage et test\n",
    "X_smote_train, X_smote_test, Y_smote_train, Y_smote_test = train_test_split(X_smote, Y_smote, test_size=0.2, random_state=42)\n",
    "\n",
    "# Création d'une pipeline avec un modèle linéaire\n",
    "pipeline = make_pipeline(lr_model(random_state=42))\n",
    "# Entrainement avec les valeurs d'apprentissage\n",
    "model = pipeline.fit(X_smote_train, Y_smote_train)\n",
    "# Calcul de la précision du modèle avec le corpus de test\n",
    "prediction = model.predict(X_smote_test)\n",
    "\n",
    "# Affichage du score\n",
    "print(f'Précision de la Régression Linéaire avec Sur-échantillonnage SMOTE {pipeline.score(X_smote_test, Y_smote_test)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ADASYN sur modèle 80/20 Régression Linéaire\n",
    "\n",
    "Référence sur l'utilisation du Sur-échantillonage à [l'addresse suivante](https://imbalanced-learn.org/stable/over_sampling.html#from-random-over-sampling-to-smote-and-adasyn)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision de la Régression Linéaire avec Sur-échantillonnage ADASYN 0.8363636363636363\n"
     ]
    }
   ],
   "source": [
    "# Création et équilibrage des échantillions\n",
    "X_as, Y_as = ADASYN().fit_resample(X, Y)\n",
    "\n",
    "# Découpage du corpus en apprentissage et test\n",
    "X_as_train, X_as_test, Y_as_train, Y_as_test = train_test_split(X_as, Y_as, test_size=0.2, random_state=42)\n",
    "\n",
    "# Création d'une pipeline avec un modèle linéaire\n",
    "pipeline = make_pipeline(lr_model(random_state=42))\n",
    "# Entrainement avec les valeurs d'apprentissage\n",
    "model = pipeline.fit(X_as_train, Y_as_train)\n",
    "# Calcul de la précision du modèle avec le corpus de test\n",
    "prediction = model.predict(X_as_test)\n",
    "\n",
    "# Affichage du score\n",
    "print(f'Précision de la Régression Linéaire avec Sur-échantillonnage ADASYN {pipeline.score(X_as_test, Y_as_test)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Indicateurs de classification"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [9071, 12430]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-26-a02c967dc0bf>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m \u001B[0maccuracy\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0maccuracy_score\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my_test\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mprediction\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      5\u001B[0m \u001B[0mprecision\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mprecision_score\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my_test\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mprediction\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[0mrecall\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mrecall_score\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my_test\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mprediction\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\hp\\documents\\github\\bitume\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py\u001B[0m in \u001B[0;36minner_f\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     61\u001B[0m             \u001B[0mextra_args\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mall_args\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     62\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mextra_args\u001B[0m \u001B[1;33m<=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 63\u001B[1;33m                 \u001B[1;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     64\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     65\u001B[0m             \u001B[1;31m# extra_args > 0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\hp\\documents\\github\\bitume\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001B[0m in \u001B[0;36maccuracy_score\u001B[1;34m(y_true, y_pred, normalize, sample_weight)\u001B[0m\n\u001B[0;32m    200\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    201\u001B[0m     \u001B[1;31m# Compute accuracy for each possible representation\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 202\u001B[1;33m     \u001B[0my_type\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_true\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_pred\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_check_targets\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my_true\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_pred\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    203\u001B[0m     \u001B[0mcheck_consistent_length\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my_true\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_pred\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msample_weight\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    204\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0my_type\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstartswith\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'multilabel'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\hp\\documents\\github\\bitume\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001B[0m in \u001B[0;36m_check_targets\u001B[1;34m(y_true, y_pred)\u001B[0m\n\u001B[0;32m     81\u001B[0m     \u001B[0my_pred\u001B[0m \u001B[1;33m:\u001B[0m \u001B[0marray\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mindicator\u001B[0m \u001B[0mmatrix\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     82\u001B[0m     \"\"\"\n\u001B[1;32m---> 83\u001B[1;33m     \u001B[0mcheck_consistent_length\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my_true\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_pred\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     84\u001B[0m     \u001B[0mtype_true\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtype_of_target\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my_true\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     85\u001B[0m     \u001B[0mtype_pred\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtype_of_target\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my_pred\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\hp\\documents\\github\\bitume\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py\u001B[0m in \u001B[0;36mcheck_consistent_length\u001B[1;34m(*arrays)\u001B[0m\n\u001B[0;32m    260\u001B[0m     \u001B[0muniques\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0munique\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlengths\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    261\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0muniques\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 262\u001B[1;33m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001B[0m\u001B[0;32m    263\u001B[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001B[0;32m    264\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: Found input variables with inconsistent numbers of samples: [9071, 12430]"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test, prediction)\n",
    "precision = precision_score(y_test, prediction)\n",
    "recall = recall_score(y_test, prediction)\n",
    "f1 = f1_score(y_test, prediction)\n",
    "\n",
    "print(f\"\"\"\n",
    "Classification par défaut\n",
    "Exactitude   : {accuracy}\n",
    "Précision    : {precision}\n",
    "Ratio Rappel : {recall}\n",
    "F-score      : {f1}\n",
    "\"\"\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modèle 80/20 Classificateur factice"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score du Classificateur factice sans distribution 0.67853599382648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hp\\documents\\github\\bitume\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Découpage du corpus en apprentissage et test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Création d'une pipeline avec un modèle linéaire\n",
    "pipeline = make_pipeline(lr_model(random_state=42))\n",
    "# Entrainement avec les valeurs d'apprentissage\n",
    "model = pipeline.fit(X_train, y_train)\n",
    "# Calcul de la précision du modèle avec le corpus de test\n",
    "prediction = model.predict(X_test)\n",
    "\n",
    "\n",
    "# Création d'une pipeline avec un modèle linéaire\n",
    "pipeline = make_pipeline(dummycl)\n",
    "# Entrainement avec les valeurs d'apprentissage\n",
    "model = pipeline.fit(X_train, y_train)\n",
    "# Calcul de la précision du modèle avec le corpus de test\n",
    "prediction = model.predict(X_test)\n",
    "\n",
    "# Affichage du score\n",
    "print(f'Score du Classificateur factice sans distribution {pipeline.score(X_test, y_test)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modèle 80/20 Gaussien"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Découpage du corpus en apprentissage et test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Création d'une pipeline avec un modèle linéaire\n",
    "pipeline = make_pipeline(gmb)\n",
    "# Entrainement avec les valeurs d'apprentissage\n",
    "model = pipeline.fit(X_train, y_train)\n",
    "# Calcul de la précision du modèle avec le corpus de test\n",
    "prediction = model.predict(X_test)\n",
    "\n",
    "# Affichage du score\n",
    "print(f'Score du modèle Gaussien sans distribution {pipeline.score(X_test, y_test)}')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Indicateurs de classification"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, prediction)\n",
    "precision = precision_score(y_test, prediction)\n",
    "recall = recall_score(y_test, prediction)\n",
    "f1 = f1_score(y_test, prediction)\n",
    "\n",
    "print(f\"\"\"\n",
    "Classification par défaut\n",
    "Exactitude   : {accuracy}\n",
    "Précision    : {precision}\n",
    "Ratio Rappel : {recall}\n",
    "F-score      : {f1}\n",
    "\"\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modèle 80/20 Arbre de décision"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Découpage du corpus en apprentissage et test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Création d'une pipeline avec un modèle linéaire\n",
    "pipeline = make_pipeline(dectree)\n",
    "# Entrainement avec les valeurs d'apprentissage\n",
    "model = pipeline.fit(X_train, y_train)\n",
    "# Calcul de la précision du modèle avec le corpus de test\n",
    "prediction = model.predict(X_test)\n",
    "\n",
    "# Affichage du score\n",
    "print(f'Score de l\\'Arbre de décision sans distribution {pipeline.score(X_test, y_test)}')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Indicateurs de classification"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, prediction)\n",
    "precision = precision_score(y_test, prediction)\n",
    "recall = recall_score(y_test, prediction)\n",
    "f1 = f1_score(y_test, prediction)\n",
    "\n",
    "print(f\"\"\"\n",
    "Classification par défaut\n",
    "Exactitude   : {accuracy}\n",
    "Précision    : {precision}\n",
    "Ratio Rappel : {recall}\n",
    "F-score      : {f1}\n",
    "\"\"\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modèle 80/20 Support Vector Machine"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Découpage train / test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Mise en place de la pipeline SVM\n",
    "pipeline = make_pipeline(svm_model)\n",
    "model = pipeline.fit(X_train, y_train)\n",
    "prediction = model.predict(X_test)\n",
    "\n",
    "print(f'Score de SVM sans distribution {pipeline.score(X_test, y_test)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### NearMiss sur modèle 80/20 Support Vecteur Machine"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# NM_X_train, NM_X_test, NM_y_train, NM_y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "#\n",
    "# NM_pipeline = make_pipeline_imb(NearMiss(random_state=42), lr_model(random_state=42))\n",
    "# NM_model = NM_pipeline.fit(NM_X_train, NM_y_train)\n",
    "# NM_prediction = NM_model.predict(NM_X_test)\n",
    "\n",
    "# print(f'Précision de Support Vecteur Machine avec Sous-échantillonnage NearMiss {NM_pipeline.score(X_test, y_test)}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Indicateurs de classification"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, prediction)\n",
    "precision = precision_score(y_test, prediction)\n",
    "recall = recall_score(y_test, prediction)\n",
    "f1 = f1_score(y_test, prediction)\n",
    "\n",
    "print(f\"\"\"\n",
    "Classification par défaut\n",
    "Exactitude   : {accuracy}\n",
    "Précision    : {precision}\n",
    "Ratio Rappel : {recall}\n",
    "F-score      : {f1}\n",
    "\"\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Mise à l'échelle et Normalisation\n",
    "## Régression Linéaire"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "min_max = preprocessing.MinMaxScaler()\n",
    "X_minmax = min_max.fit_transform(X)\n",
    "\n",
    "std_scale = preprocessing.StandardScaler()\n",
    "X_scale = std_scale.fit_transform(X)\n",
    "\n",
    "X_l1 = preprocessing.normalize(X, norm=\"l1\")\n",
    "X_l2 = preprocessing.normalize(X, norm=\"l2\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Calcul de la MAE, MSE, RMSE sur une Régression Linéaire\n",
    "\n",
    "https://towardsdatascience.com/linear-regression-in-python-9a1f5f000606\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html#mean-squared-error"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_lr_model(X_dat):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_dat, Y, test_size=0.2, random_state=42)\n",
    "    pipeline = make_pipeline(lr_model())\n",
    "    model = pipeline.fit(X_train, y_train)\n",
    "    return X_test, y_test, model\n",
    "\n",
    "X_test, y_test, model = get_lr_model(X)\n",
    "prediction = model.predict(X_test)\n",
    "print(f\"MAE sur une Régression Linéaire sans normalisation {mean_absolute_error(y_test, prediction)}\")\n",
    "print(f\"MSE sur une Régression Linéaire sans normalisation {mean_squared_error(y_test, prediction)}\")\n",
    "print(f\"RMSE sur une Régression Linéaire sans normalisation {np.sqrt(mean_squared_error(y_test, prediction))}\")\n",
    "print(\"\")\n",
    "\n",
    "# X_test, y_test, model = get_lr_model(X_minmax)\n",
    "# prediction = model.predict(X_test)\n",
    "# print(f\"MSE sur une Régression Linéaire avec MinMax {mean_squared_error(y_test, prediction)}\")\n",
    "#\n",
    "X_test, y_test, model = get_lr_model(X_scale)\n",
    "prediction = model.predict(X_test)\n",
    "print(f\"MAE sur une Régression Linéaire avec Répartitions Normales {mean_absolute_error(y_test, prediction)}\")\n",
    "print(f\"MSE sur une Régression Linéaire avec Répartitions Normales {mean_squared_error(y_test, prediction)}\")\n",
    "print(f\"RMSE sur une Régression Linéaire avec Répartitions Normales {np.sqrt(mean_squared_error(y_test, prediction))}\")\n",
    "print(\"\")\n",
    "\n",
    "X_test, y_test, model = get_lr_model(X_l1)\n",
    "prediction = model.predict(X_test)\n",
    "print(f\"MAE sur une Régression Linéaire avec Normalisation L1 {mean_absolute_error(y_test, prediction)}\")\n",
    "print(f\"MSE sur une Régression Linéaire avec Normalisation L1 {mean_squared_error(y_test, prediction)}\")\n",
    "print(f\"RMSE sur une Régression Linéaire avec Normalisation L1 {np.sqrt(mean_squared_error(y_test, prediction))}\")\n",
    "print(\"\")\n",
    "\n",
    "X_test, y_test, model = get_lr_model(X_l2)\n",
    "prediction = model.predict(X_test)\n",
    "print(f\"MAE sur une Régression Linéaire avec Normalisation L2 {mean_absolute_error(y_test, prediction)}\")\n",
    "print(f\"MSE sur une Régression Linéaire avec Normalisation L2 {mean_squared_error(y_test, prediction)}\")\n",
    "print(f\"RMSE sur une Régression Linéaire avec Normalisation L2 {np.sqrt(mean_squared_error(y_test, prediction))}\")\n",
    "print(\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exactitude après normaliastion et mise à l'échelle"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "pipeline = make_pipeline(lr_model(random_state=42))\n",
    "model = pipeline.fit(X_train, y_train)\n",
    "prediction = model.predict(X_test)\n",
    "\n",
    "print(f'Exactitude de la Régression Linéaire sans distribution {pipeline.score(X_test, y_test)}')\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_minmax, Y, test_size=0.2, random_state=42)\n",
    "#\n",
    "# pipeline = make_pipeline(lr_model(random_state=42))\n",
    "# model = pipeline.fit(X_train, y_train)\n",
    "# prediction = model.predict(X_test)\n",
    "#\n",
    "# print(f'Exactitude de la Régression Linéaire avec MinMax {pipeline.score(X_test, y_test)}')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scale, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "pipeline = make_pipeline(lr_model(random_state=42))\n",
    "model = pipeline.fit(X_train, y_train)\n",
    "prediction = model.predict(X_test)\n",
    "\n",
    "print(f'Exactitude de la Régression Linéaire avec Répartitions Normales {pipeline.score(X_test, y_test)}')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_l1, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "pipeline = make_pipeline(lr_model(random_state=42))\n",
    "model = pipeline.fit(X_train, y_train)\n",
    "prediction = model.predict(X_test)\n",
    "\n",
    "print(f'Exactitude de la Régression Linéaire avec Normalisation L1 {pipeline.score(X_test, y_test)}')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_l2, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "pipeline = make_pipeline(lr_model(random_state=42))\n",
    "model = pipeline.fit(X_train, y_train)\n",
    "prediction = model.predict(X_test)\n",
    "\n",
    "print(f'Exactitude de la Régression Linéaire avec Normalisation L2 {pipeline.score(X_test, y_test)}')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Support Vecteur Machine\n",
    "\n",
    "Note: j'ai pris la décision de ne *pas* réaliser ces tests en raison de mon toaster"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: code"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Validation Croisée\n",
    "\n",
    "## Validation en 10-étapes (kFolds)\n",
    "On fixe la valeur de k à k=10, qui est connue pour être la plus compêtante en général.\n",
    "Source: https://machinelearningmastery.com/k-fold-cross-validation/\n",
    "\n",
    "### Régression Linéaire"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def calc_accuracy_score(model, name, X, Y):\n",
    "    scores = cross_val_score(model, X, Y, cv=10)\n",
    "    scores_precision = cross_val_score(model, X, Y, cv=10, scoring='precision')\n",
    "    scores_recall = cross_val_score(model, X, Y, cv=10, scoring='recall')\n",
    "    scores_f1 = cross_val_score(model, X, Y, cv=10, scoring='f1')\n",
    "    print(f\"\"\"\n",
    "Évaluation de {name} sous Validation Croisée 10-étapes\n",
    "\n",
    "Exactitude   : {scores.mean():.3f} (+/- {scores.std() * 2:.3f})\n",
    "Précision    : {scores_precision.mean():.3f} (+/- {scores_precision.std() * 2:.3f})\n",
    "Ratio Rappel : {scores_recall.mean():.3f} (+/- {scores_recall.std() * 2:.3f})\n",
    "F-score      : {scores_f1.mean():.3f} (+/- {scores_f1.std() * 2:.3f})\n",
    "    \"\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = lr_model()\n",
    "calc_accuracy_score(model, \"la Régression Linéaire\", X, Y)\n",
    "# calc_accuracy_score(model, \"la Régression Linéaire avec MinMax\", X_minmax, Y)\n",
    "# calc_accuracy_score(model, \"la Régression Linéaire avec Scaler\", X_scale, Y)\n",
    "# calc_accuracy_score(model, \"la Régression Linéaire avec Normalize 1\", X_l1, Y)\n",
    "# calc_accuracy_score(model, \"la Régression Linéaire avec Normalize 2\", X_l2, Y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Classificateur factice"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "calc_accuracy_score(dummycl, \"l'application du Classificateur factice\", X, Y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Gaussien"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "calc_accuracy_score(gmb, \"la classification Gaussienne\", X, Y)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Arbre de décision"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "calc_accuracy_score(dectree, \"l'Arbre de décision\", X, Y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Support Vecteur Machine"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Je n'ai volontairement pas executé les calculs suivants afin de ne pas achever mon toaster\n",
    "\n",
    "lsvm_model = SVC()\n",
    "calc_accuracy_score(lsvm_model, \"Support Vecteur Machine\", X, Y)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Modèles retenus"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nm1 = NearMiss(version=1)\n",
    "\n",
    "# Équilibrage des échantillions\n",
    "X_resampled_nm1, Y_resampled = nm1.fit_resample(X, Y)\n",
    "\n",
    "model = LogisticRegression(random_state=42)\n",
    "\n",
    "calc_accuracy_score(model, \"la Régression Linéaire avec NearMiss\", X_resampled_nm1, Y_resampled)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nm1 = NearMiss(version=1)\n",
    "\n",
    "std_scale = preprocessing.StandardScaler()\n",
    "X_scale = std_scale.fit_transform(X)\n",
    "\n",
    "# Équilibrage des échantillions\n",
    "X_resampled_nm1, Y_resampled = nm1.fit_resample(X_scale, Y)\n",
    "\n",
    "model = LogisticRegression(random_state=42)\n",
    "\n",
    "calc_accuracy_score(model, \"la Régression Linéaire avec NearMiss et StandardScaler\", X_resampled_nm1, Y_resampled)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}